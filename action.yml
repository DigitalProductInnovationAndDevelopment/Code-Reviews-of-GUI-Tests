# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
#  GUI-BASED TESTING CODE REVIEW ‚Äî MODULAR COMPOSITE ACTION
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
name: 'GUI-Based Testing Code Review'
description: 'Playwright GUI tests ¬∑ ESLint/Prettier ¬∑ visual dashboard, checklist & PR comment'
author: 'Digital Product Innovation and Development ‚Äì TUM'

branding:
  icon: 'eye'
  color: 'purple'

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# ‚ñ∏‚ñ∏ Inputs
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
inputs:
  github-token:
    description: 'GitHub token (or PAT) for API calls'
    required: true
    default: ${{ github.token }}

  mode:
    description: |
      full            ‚Äì run lint, tests, dashboard   (default)
      lint-only       ‚Äì only ESLint/Prettier
      test-only       ‚Äì only Playwright
      dashboard-only  ‚Äì build dashboard from existing artifacts
    required: false
    default: 'full'

  enable-playwright:
    description: 'Run Playwright (overrides mode)'
    required: false
    default: 'true'

  enable-lint:
    description: 'Run ESLint / Prettier (overrides mode)'
    required: false
    default: 'true'

  enable-dashboard:
    description: 'Generate dashboard / checklist (overrides mode)'
    required: false
    default: 'true'

  enable-pr-comments:
    description: 'Post summary comment on pull-requests'
    required: false
    default: 'true'

  enable-visual-comparison:
    description: 'Also run Playwright on the main branch and compare'
    required: false
    default: 'false'

  test-files:
    description: 'Folder or glob passed to "npx playwright test"'
    required: false
    default: 'tests'

  node-version:
    description: 'Node version'
    required: false
    default: '18'

  playwright-config:
    description: 'Path to Playwright config'
    required: false
    default: 'playwright.config.js'

  reviewdog-reporter:
    description: 'Reviewdog reporter'
    required: false
    default: 'github-pr-review'

  main-branch:
    description: 'Main branch for comparison'
    required: false
    default: 'main'

  key-test-file:
    description: 'File used to verify checkout'
    required: false
    default: 'tests/demo-todo-app.spec.ts'

  enable-github-pages:
    description: 'Deploy dashboard to GitHub Pages'
    required: false
    default: 'true'

  web-report-url:
    description: 'Base URL override for dashboard link'
    required: false
    default: ''

  artifacts-retention-days:
    description: 'Days to retain artifacts'
    required: false
    default: '30'

  fail-on-test-failure:
    description: 'Fail job if Playwright tests fail'
    required: false
    default: 'false'

  extra-npm-dependencies:
    description: 'Extra npm packages (space-separated)'
    required: false
    default: ''

  custom-artifacts-path:
    description: |
      Dashboard-only: folder already containing artifacts.
      Must include:
        ‚Ä¢ playwright-summary*.json
        ‚Ä¢ playwright-report/ (HTML & screenshots)
    required: false
    default: ''

  max-test-retries:
    description: 'Maximum number of retries for flaky tests'
    required: false
    default: '2'

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# ‚ñ∏‚ñ∏ Outputs (Enhanced with new outputs)
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
outputs:
  test-results:
    description: 'JSON summary of Playwright results'
    value: ${{ steps.test-summary.outputs.results }}
  visual-artifacts-path:
    description: 'Path to generated artifacts'
    value: ${{ steps.setup.outputs.artifacts-path }}
  dashboard-url:
    description: 'Deployed dashboard URL'
    value: ${{ steps.deploy-report.outputs.page_url }}
  test-pass-rate:
    description: 'Playwright pass-rate'
    value: ${{ steps.test-summary.outputs.pass-rate }}
  total-gui-tests:
    description: 'Total Playwright tests executed'
    value: ${{ steps.test-summary.outputs.total-tests }}
  code-quality-score:
    description: 'Overall quality score (lint)'
    value: ${{ steps.lint-summary.outputs.quality-score }}
  pr-test-results:
    description: 'PR-branch Playwright summary'
    value: ${{ steps.test-summary.outputs.pr-results }}
  main-test-results:
    description: 'Main-branch Playwright summary'
    value: ${{ steps.test-summary.outputs.main-results }}
  visual-comparison:
    description: 'Comparison PR vs main'
    value: ${{ steps.test-summary.outputs.visual-comparison }}
  gui-regression-detected:
    description: 'True if regression detected'
    value: ${{ steps.test-summary.outputs.regression-detected }}
  review-checklist-status:
    description: 'Checklist completion status'
    value: ${{ steps.checklist.outputs.status }}
  has-failures:
    description: 'Boolean indicating if any tests failed'
    value: ${{ steps.test-summary.outputs.has-failures }}
  failure-details:
    description: 'JSON array of failed test names'
    value: ${{ steps.test-summary.outputs.failure-details }}
  execution-time:
    description: 'Total execution time in seconds'
    value: ${{ steps.timing.outputs.duration }}
  performance-metrics:
    description: 'JSON object with performance data'
    value: ${{ steps.performance.outputs.metrics }}

# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
# ‚ñ∏‚ñ∏ Composite run
# ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
runs:
  using: composite
  steps:

  # 0a - Start timing
  - id: timing
    name: Start execution timer
    shell: bash
    run: |
      echo "start-time=$(date +%s)" >> $GITHUB_OUTPUT
      echo "üèÅ Starting GUI Test Review at $(date)"

  # 0b - derive feature flags
  - id: modes
    name: Derive feature flags
    shell: bash
    run: |
      play='${{ inputs.enable-playwright }}'
      lint='${{ inputs.enable-lint }}'
      dash='${{ inputs.enable-dashboard }}'
      compare='${{ inputs.enable-visual-comparison }}'
      case '${{ inputs.mode }}' in
        lint-only)      play=false ; dash=false ; compare=false ;;
        test-only)      lint=false ; dash=false ;;
        dashboard-only) play=false ; lint=false ; compare=false ;;
      esac
      echo "playwright=$play" >> $GITHUB_OUTPUT
      echo "lint=$lint"       >> $GITHUB_OUTPUT
      echo "dashboard=$dash"  >> $GITHUB_OUTPUT
      echo "compare=$compare" >> $GITHUB_OUTPUT

  # 1 workspace
  - id: setup
    name: Prepare workspace
    shell: bash
    run: |
      echo "üìÅ Creating workspace directories..."
      mkdir -p artifacts
      echo "artifacts-path=artifacts" >> $GITHUB_OUTPUT
      
  # 1b - Validate inputs early
  - name: Validate configuration
    shell: bash
    run: |
      echo "üîç Validating configuration..."
      
      # Check if tests directory/files exist
      if [ ! -d "${{ inputs.test-files }}" ] && ! ls ${{ inputs.test-files }} 2>/dev/null; then
        echo "‚ö†Ô∏è  Warning: No test files found matching '${{ inputs.test-files }}'"
        echo "‚ö†Ô∏è  This may cause Playwright tests to fail if mode includes testing"
      else
        echo "‚úÖ Test files/directory found: ${{ inputs.test-files }}"
      fi
      
      # Warn if no package.json
      if [ ! -f package.json ]; then
        echo "‚ö†Ô∏è  Warning: No package.json found. Using defaults for all configurations."
        echo "‚ö†Ô∏è  Some features may not work as expected without proper dependencies."
      else
        echo "‚úÖ package.json found"
      fi
      
      # Check playwright config
      if [ "${{ steps.modes.outputs.playwright }}" == "true" ]; then
        if [ ! -f "${{ inputs.playwright-config }}" ]; then
          echo "‚ö†Ô∏è  Warning: Playwright config not found at '${{ inputs.playwright-config }}'"
          echo "‚ö†Ô∏è  Will use default configuration"
        else
          echo "‚úÖ Playwright config found: ${{ inputs.playwright-config }}"
        fi
      fi
      
  - name: Setup default configs
    shell: bash
    run: |
      # Get action's directory path
      ACTION_PATH="${{ github.action_path }}"
      
      # Check and copy Playwright config if missing
      if [ ! -f "playwright.config.js" ] && [ ! -f "playwright.config.ts" ]; then
        echo "üìã No Playwright config found, using default..."
        cp "$ACTION_PATH/playwright.config.js" ./playwright.config.js
      fi
      
      # Check and copy ESLint config if missing
      if [ ! -f ".eslintrc.json" ] && [ ! -f ".eslintrc.js" ] && [ ! -f "eslint.config.js" ] && [ ! -f "eslint.config.mjs" ]; then
        echo "üìã No ESLint config found, using default..."
        # Copy the traditional format (since it's more compatible)
        cp "$ACTION_PATH/.eslintrc.json" ./.eslintrc.json
      fi
      
      # Check and copy Prettier config if missing  
      if [ ! -f ".prettierrc" ] && [ ! -f ".prettierrc.json" ] && [ ! -f "prettier.config.js" ]; then
        echo "üìã No Prettier config found, using default..."
        cp "$ACTION_PATH/.prettierrc.json" ./.prettierrc.json
      fi
      
  - uses: actions/setup-node@v4
    with:
      node-version: ${{ inputs.node-version }}

  # 1c - Cache action dependencies (new)
  - name: Cache action dependencies
    uses: actions/cache@v3
    with:
      path: |
        ~/.npm
        node_modules
      key: ${{ runner.os }}-action-deps-${{ hashFiles('**/package-lock.json', '**/package.json') }}
      restore-keys: |
        ${{ runner.os }}-action-deps-

  - name: Install npm dependencies
    shell: bash
    run: |
      echo "üì¶ Installing dependencies..."
      # Install user dependencies if package.json exists
      if [ -f package.json ]; then 
        echo "üì¶ Installing project dependencies..."
        npm ci 2>/dev/null || npm install
      fi
      
      # Always install action's required runtime dependencies
      echo "üì¶ Installing action dependencies..."
      npm install --no-save marked@15.0.12 @octokit/core@^5.0.0 @mermaid-js/mermaid-cli@10.6.1
      
      # Install any extra user-specified dependencies
      if [ -n "${{ inputs.extra-npm-dependencies }}" ]; then
        echo "üì¶ Installing extra dependencies: ${{ inputs.extra-npm-dependencies }}"
        npm install --no-save ${{ inputs.extra-npm-dependencies }}
      fi
      
      # Create puppeteer config for mermaid-cli
      echo '{ "args": ["--no-sandbox", "--disable-setuid-sandbox"] }' > puppeteer.json

  # 2 lint
  - uses: reviewdog/action-setup@v1
    if: steps.modes.outputs.lint == 'true' && github.event_name == 'pull_request'
    with:
      reviewdog_version: latest

  - name: ESLint / Prettier
    if: steps.modes.outputs.lint == 'true'
    shell: bash
    env:
      NODE_PATH: ${{ github.workspace }}/node_modules
      GITHUB_TOKEN: ${{ inputs.github-token }}
    run: |
      echo "üé® Running code quality checks..."
      echo "üìè Checking ESLint..."
      echo "üé® Checking Prettier formatting..."
      ACTION_PATH="${{ github.action_path }}"
      if [ -f "$ACTION_PATH/scripts/lint.js" ]; then 
        node "$ACTION_PATH/scripts/lint.js"
      elif npx --no-install eslint -v &>/dev/null; then 
        npx eslint .
      else 
        echo '‚ÑπÔ∏è no lint script'
      fi
    continue-on-error: true

  # 3 ‚îÄ‚îÄ Playwright on PR branch (with retry logic)
  - name: Install Playwright browsers
    if: steps.modes.outputs.playwright == 'true'
    shell: bash
    run: |
      echo "üé≠ Installing Playwright browsers..."
      npx playwright install --with-deps

  - name: Run Playwright (PR) with retry
    if: steps.modes.outputs.playwright == 'true'
    shell: bash
    env:
      PLAYWRIGHT_CONFIG: ${{ inputs.playwright-config }}
    run: |
      echo "üß™ Running Playwright tests on PR branch..."
      echo "üìÅ Test files: ${{ inputs.test-files }}"
      echo "‚è±Ô∏è  This may take a few minutes..."
      
      MAX_RETRIES=${{ inputs.max-test-retries }}
      RETRY_COUNT=0
      
      until npx playwright test "${{ inputs.test-files }}" || [ $RETRY_COUNT -eq $MAX_RETRIES ]; do
        RETRY_COUNT=$((RETRY_COUNT+1))
        echo "‚ö†Ô∏è  Tests failed, retry $RETRY_COUNT of $MAX_RETRIES..."
        sleep 5
      done
      
      if [ $RETRY_COUNT -eq $MAX_RETRIES ] && [ ! -f playwright-metrics.json ]; then
        echo "‚ùå Tests failed after $MAX_RETRIES retries"
      else
        echo "‚úÖ PR tests completed!"
      fi
    continue-on-error: true
    
  - name: Archive Playwright artifacts (PR)
    if: steps.modes.outputs.playwright == 'true'
    shell: bash
    run: |
      [ -f playwright-summary.json ] && mv playwright-summary.json artifacts/playwright-summary-pr.json
      [ -f playwright-metrics.json ] && mv playwright-metrics.json artifacts/
      if [ -d playwright-report ]; then
        mkdir -p artifacts/pr-report
        mv playwright-report/* artifacts/pr-report/
      fi
      if [ ! -f artifacts/playwright-summary-pr.json ] && [ -f artifacts/playwright-metrics.json ]; then
        jq '
          # count helper ‚Äî walks the whole tree
          def count($f): [ .. | objects | select($f) ] | length ;
        
          # derive numbers with portable `as` bindings
          ( .stats.passed  // .stats.passes   // .stats.expected
            // count(.outcome=="expected"  or .status=="passed") )        as $passed
        | ( .stats.failed  // .stats.failures // .stats.unexpected
            // count(.outcome=="unexpected" or .status=="failed") )       as $failed
        | ( .stats.skipped // count(.outcome=="skipped"  or .status=="skipped") ) as $skipped
        | ($passed + $failed + $skipped)                                  as $total
        
        | {
            total:    $total,
            passed:   $passed,
            failed:   $failed,
            skipped:  $skipped,
            duration: (.stats.duration // 0),
            pass_rate: ( if $total==0 then 0
                         else ( ($passed * 100) / $total | floor ) end )
          }
        '  artifacts/playwright-metrics.json > artifacts/playwright-summary-pr.json

      fi

  - name: Alias summary for checklist
    if: steps.modes.outputs.playwright == 'true'
    shell: bash
    run: |
      cp artifacts/playwright-summary-pr.json artifacts/playwright-summary.json || true

  # 3b ‚îÄ‚îÄ Playwright on main branch with progress indicators
  - name: Run Playwright on main
    if: steps.modes.outputs.compare == 'true' && github.event_name == 'pull_request'
    shell: bash
    env:
      PLAYWRIGHT_CONFIG: ${{ inputs.playwright-config }}
    run: |
      echo "üîÑ Fetching main branch for comparison..."
      git fetch --quiet origin "${{ inputs.main-branch }}" || true
      git checkout origin/${{ inputs.main-branch }} -- tests/ playwright.config.js || true

      echo "üß™ Running Playwright tests on main branch..."
      echo "‚è±Ô∏è  This helps detect regressions..."
      
      npx playwright test "${{ inputs.test-files }}"

      mkdir -p artifacts/main-report
      [ -d playwright-report ] && mv playwright-report/* artifacts/main-report/ || true
      [ -f playwright-summary.json ] && mv playwright-summary.json artifacts/playwright-summary-main.json || true
      [ -f playwright-metrics.json ] && mv playwright-metrics.json artifacts/

      if [ ! -f artifacts/playwright-summary-main.json ] && [ -f artifacts/playwright-metrics.json ]; then
        jq '
          # count helper ‚Äî walks the whole tree
          def count($f): [ .. | objects | select($f) ] | length ;
        
          # derive numbers with portable `as` bindings
          ( .stats.passed  // .stats.passes   // .stats.expected
            // count(.outcome=="expected"  or .status=="passed") )        as $passed
        | ( .stats.failed  // .stats.failures // .stats.unexpected
            // count(.outcome=="unexpected" or .status=="failed") )       as $failed
        | ( .stats.skipped // count(.outcome=="skipped"  or .status=="skipped") ) as $skipped
        | ($passed + $failed + $skipped)                                  as $total
        
        | {
            total:    $total,
            passed:   $passed,
            failed:   $failed,
            skipped:  $skipped,
            duration: (.stats.duration // 0),
            pass_rate: ( if $total==0 then 0
                         else ( ($passed * 100) / $total | floor ) end )
          }
        '  artifacts/playwright-metrics.json > artifacts/playwright-summary-main.json

      fi

      git checkout HEAD -- tests/ playwright.config.js
      echo "‚úÖ Main branch tests completed!"
    continue-on-error: true

  # 4 external artifacts (dashboard-only)
  - name: Import external artifacts
    if: inputs.mode == 'dashboard-only'
    shell: bash
    run: |
      SRC='${{ inputs.custom-artifacts-path }}'
      if [ "$SRC" != 'artifacts' ] && [ -d "$SRC" ]; then
        cp -R "$SRC/." artifacts/
      fi

  # 5 ensure metrics for flow-chart
  - name: Locate metrics JSON
    if: steps.modes.outputs.dashboard == 'true'
    shell: bash
    run: |
      M=$(find artifacts -name playwright-metrics.json | head -n1 || true)
      [ -n "$M" ] && cp "$M" playwright-metrics.json

  # 6 flow-chart
  - name: Flow-chart
    if: steps.modes.outputs.dashboard == 'true'
    shell: bash
    env:
      NODE_PATH: ${{ github.workspace }}/node_modules
    run: |
      echo "üìä Generating test flow visualization..."
      ACTION_PATH="${{ github.action_path }}"
      [ -f "$ACTION_PATH/scripts/generate-flowchart.js" ] && node "$ACTION_PATH/scripts/generate-flowchart.js"
    continue-on-error: true

  # 7 checklist
  - id: checklist
    if: steps.modes.outputs.dashboard == 'true'
    shell: bash
    env:
      NODE_PATH: ${{ github.workspace }}/node_modules
    run: |
      echo "‚úÖ Generating review checklist..."
      ACTION_PATH="${{ github.action_path }}"
      if [ -f "$ACTION_PATH/scripts/checklist.js" ]; then
        node "$ACTION_PATH/scripts/checklist.js"
        # Extract checklist status
        if [ -f artifacts/checklist.json ]; then
          STATUS=$(jq -r '.status // "unknown"' artifacts/checklist.json 2>/dev/null || echo "unknown")
          echo "status=$STATUS" >> $GITHUB_OUTPUT
        fi
      else
        echo '{"status":"missing"}' > artifacts/checklist.json
      fi
    continue-on-error: true

  # 7b - Analyze test failures
  - name: Analyze test failures
    if: steps.modes.outputs.dashboard == 'true' && steps.test-summary.outputs.has-failures == 'true'
    shell: bash
    env:
      NODE_PATH: ${{ github.workspace }}/node_modules
    run: |
      echo "üîç Analyzing test failure patterns..."
      ACTION_PATH="${{ github.action_path }}"
      # Create the analysis script if it doesn't exist in the action
      if [ ! -f "$ACTION_PATH/scripts/analyze-test-failures.js" ]; then
        echo "‚ö†Ô∏è  Test failure analyzer not found in action, skipping analysis"
      else
        node "$ACTION_PATH/scripts/analyze-test-failures.js"
      fi
    continue-on-error: true

  # 8 dashboard HTML with performance metrics
  - id: dashboard
    name: Build dashboard HTML
    if: steps.modes.outputs.dashboard == 'true'
    shell: bash
    env:
      NODE_PATH: ${{ github.workspace }}/node_modules
      ACTION_START_TIME: ${{ steps.timing.outputs.start-time }}
    run: |
      echo "üé® Building interactive dashboard..."
      ACTION_PATH="${{ github.action_path }}"
      # Export timing for performance metrics
      export DASHBOARD_GEN_START=$(date +%s%N)
      [ -f "$ACTION_PATH/scripts/generate-webpage.js" ] && node "$ACTION_PATH/scripts/generate-webpage.js"
    continue-on-error: true

  # 8b - Collect performance metrics
  - id: performance
    name: Collect performance metrics
    if: steps.modes.outputs.dashboard == 'true'
    shell: bash
    run: |
      END_TIME=$(date +%s)
      START_TIME=${{ steps.timing.outputs.start-time }}
      DURATION=$((END_TIME - START_TIME))
      
      # Calculate artifact sizes
      TOTAL_SIZE=0
      if [ -d artifacts ]; then
        TOTAL_SIZE=$(du -sb artifacts | cut -f1 | awk '{print $1/1048576}')
      fi
      
      # Create performance metrics JSON
      cat > artifacts/performance-metrics.json <<EOF
      {
        "executionTime": $DURATION,
        "artifactSizeMB": $TOTAL_SIZE,
        "timestamp": "$(date -u +%Y-%m-%dT%H:%M:%SZ)"
      }
      EOF
      
      echo "metrics=$(cat artifacts/performance-metrics.json | jq -c .)" >> $GITHUB_OUTPUT

  # 9 upload Pages artifact & deploy
  - uses: actions/upload-pages-artifact@v3
    if: steps.modes.outputs.dashboard == 'true' && inputs.enable-github-pages == 'true'
    with:
      name: gui-test-visual-artifacts
      path: artifacts/web-report

  - id: deploy-report
    name: Deploy to GitHub Pages
    if: steps.modes.outputs.dashboard == 'true' && inputs.enable-github-pages == 'true'
    uses: actions/deploy-pages@v4
    with:
      artifact_name: gui-test-visual-artifacts
    continue-on-error: true

  # 10 - Add test summary output to GitHub Actions
  - name: Output test summary
    if: steps.modes.outputs.playwright == 'true'
    shell: bash
    run: |
      if [ -f artifacts/playwright-summary-pr.json ]; then
        echo "## üß™ Test Results" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "| Metric | Value |" >> $GITHUB_STEP_SUMMARY
        echo "|--------|-------|" >> $GITHUB_STEP_SUMMARY
        
        # Parse JSON and create summary
        jq -r '. | "| Total Tests | \(.total) |\n| ‚úÖ Passed | \(.passed) |\n| ‚ùå Failed | \(.failed) |\n| ‚è≠Ô∏è Skipped | \(.skipped) |\n| üìä Pass Rate | \(.pass_rate)% |\n| ‚è±Ô∏è Duration | \(.duration)ms |"' artifacts/playwright-summary-pr.json >> $GITHUB_STEP_SUMMARY
        
        # Add link to dashboard if available
        if [ "${{ steps.deploy-report.outputs.page_url }}" != "" ]; then
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "üîó [View Full Dashboard](${{ steps.deploy-report.outputs.page_url }})" >> $GITHUB_STEP_SUMMARY
        fi
      fi

  # 10b - Calculate test summary outputs
  - id: test-summary
    name: Calculate test summary outputs
    if: steps.modes.outputs.playwright == 'true' || inputs.mode == 'dashboard-only'
    shell: bash
    run: |
      # Read PR summary
      if [ -f artifacts/playwright-summary-pr.json ]; then
        PR_RESULTS=$(cat artifacts/playwright-summary-pr.json | jq -c .)
        TOTAL=$(echo "$PR_RESULTS" | jq -r '.total // 0')
        PASSED=$(echo "$PR_RESULTS" | jq -r '.passed // 0')
        FAILED=$(echo "$PR_RESULTS" | jq -r '.failed // 0')
        PASS_RATE=$(echo "$PR_RESULTS" | jq -r '.pass_rate // 0')
        
        echo "results=$PR_RESULTS" >> $GITHUB_OUTPUT
        echo "pr-results=$PR_RESULTS" >> $GITHUB_OUTPUT
        echo "total-tests=$TOTAL" >> $GITHUB_OUTPUT
        echo "pass-rate=$PASS_RATE" >> $GITHUB_OUTPUT
        echo "has-failures=$([[ $FAILED -gt 0 ]] && echo 'true' || echo 'false')" >> $GITHUB_OUTPUT
        
        # Extract failed test names if any
        if [ -f artifacts/playwright-metrics.json ] && [ $FAILED -gt 0 ]; then
          FAILED_TESTS=$(jq -c '[.. | objects | select(.status == "failed" or .outcome == "unexpected") | .title // .name // "Unknown test"] | unique' artifacts/playwright-metrics.json 2>/dev/null || echo '[]')
          echo "failure-details=$FAILED_TESTS" >> $GITHUB_OUTPUT
        else
          echo "failure-details=[]" >> $GITHUB_OUTPUT
        fi
      fi
      
      # Read main summary if exists
      if [ -f artifacts/playwright-summary-main.json ]; then
        MAIN_RESULTS=$(cat artifacts/playwright-summary-main.json | jq -c .)
        echo "main-results=$MAIN_RESULTS" >> $GITHUB_OUTPUT
        
        # Calculate regression
        if [ -f artifacts/playwright-summary-pr.json ]; then
          PR_FAILED=$(echo "$PR_RESULTS" | jq -r '.failed // 0')
          MAIN_FAILED=$(echo "$MAIN_RESULTS" | jq -r '.failed // 0')
          echo "regression-detected=$([[ $PR_FAILED -gt $MAIN_FAILED ]] && echo 'true' || echo 'false')" >> $GITHUB_OUTPUT
        fi
      fi

  # 11 PR comment
  - name: PR summary comment
    if: steps.modes.outputs.dashboard == 'true' &&
        inputs.enable-pr-comments == 'true' &&
        github.event_name == 'pull_request'
    shell: bash
    env:
      GITHUB_TOKEN: ${{ inputs.github-token }}
      WEB_REPORT_URL: ${{ inputs.web-report-url || steps.deploy-report.outputs.page_url }}
      NODE_PATH: ${{ github.workspace }}/node_modules
    run: |
      echo "üí¨ Posting summary comment to PR..."
      ACTION_PATH="${{ github.action_path }}"
      [ -f "$ACTION_PATH/scripts/summary-comment.js" ] && node "$ACTION_PATH/scripts/summary-comment.js"
    continue-on-error: true

  # 12 - Calculate final timing
  - name: Calculate execution time
    id: timing-final
    shell: bash
    run: |
      END_TIME=$(date +%s)
      START_TIME=${{ steps.timing.outputs.start-time }}
      DURATION=$((END_TIME - START_TIME))
      echo "duration=$DURATION" >> $GITHUB_OUTPUT
      echo "‚è±Ô∏è  Total execution time: ${DURATION}s"

  # 13 - Finish with better messaging
  - name: Finish
    shell: bash
    run: |
      echo '‚úÖ GUI-Based Testing Code Review completed!'
      
      # Provide helpful summary based on results
      if [ -f artifacts/playwright-summary-pr.json ]; then
        FAILED=$(jq -r '.failed // 0' artifacts/playwright-summary-pr.json)
        if [ "$FAILED" -gt "0" ]; then
          echo "‚ùå $FAILED test(s) failed. Check the dashboard for details."
        else
          echo "‚úÖ All tests passed!"
        fi
      fi
      
      if [ "${{ steps.deploy-report.outputs.page_url }}" != "" ]; then
        echo "üìä Dashboard: ${{ steps.deploy-report.outputs.page_url }}"
      fi

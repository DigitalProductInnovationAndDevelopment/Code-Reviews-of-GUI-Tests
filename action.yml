name: 'GUI-Based Testing Code Review'
description: 'Enhance code review process for automated GUI tests by integrating visual context directly into GitHub pull requests'
author: 'Digital Product Innovation and Development - TUM'

branding:
  icon: 'eye'
  color: 'purple'

inputs:
  github-token:
    description: 'GitHub token for API access and reviewdog integration'
    required: true
    default: ${{ github.token }}
  
  mode:
    description: 'Operation mode: full, lint-only, test-only, dashboard-only, comment-only'
    required: false
    default: 'full'
  
  test-files:
    description: 'Glob pattern for GUI test files to analyze'
    required: false
    default: 'tests/**/*.spec.{js,ts,tsx}'
  
  node-version:
    description: 'Node.js version to use'
    required: false
    default: '18'
  
  enable-pr-comments:
    description: 'Whether to post comprehensive PR comments with visual feedback'
    required: false
    default: 'true'
  
  enable-github-pages:
    description: 'Whether to deploy visual dashboard to GitHub Pages'
    required: false
    default: 'true'
  
  reviewdog-reporter:
    description: 'Reviewdog reporter for inline code quality feedback'
    required: false
    default: 'github-pr-review'
  
  artifacts-retention-days:
    description: 'Number of days to retain test artifacts and screenshots'
    required: false
    default: '30'
  
  web-report-url:
    description: 'Base URL for the visual dashboard (auto-generated if not provided)'
    required: false
    default: ''
  
  fail-on-test-failure:
    description: 'Whether to fail the action if GUI tests fail'
    required: false
    default: 'false'
  
  playwright-config:
    description: 'Path to Playwright configuration file'
    required: false
    default: 'playwright.config.js'
  
  enable-visual-comparison:
    description: 'Enable visual comparison between PR and main branch test results'
    required: false
    default: 'true'
  
  main-branch:
    description: 'Main branch name for visual comparison testing'
    required: false
    default: 'main'
  
  key-test-file:
    description: 'Key test file to verify successful main branch checkout'
    required: false
    default: 'tests/demo-todo-app.spec.ts'
  
  # Modular Inputs
  playwright-artifact:
    description: 'Name of existing Playwright artifact to use (dashboard-only mode)'
    required: false
    default: ''
  
  eslint-artifact:
    description: 'Name of existing ESLint artifact to use (dashboard-only mode)'
    required: false
    default: ''
  
  prettier-artifact:
    description: 'Name of existing Prettier artifact to use (dashboard-only mode)'
    required: false
    default: ''
  
  custom-artifacts-path:
    description: 'Path to custom artifacts directory'
    required: false
    default: 'artifacts'
  
  dependencies:
    description: 'Additional npm dependencies to install (space-separated)'
    required: false
    default: ''
  
  use-project-eslint:
    description: 'Use project ESLint config if exists'
    required: false
    default: 'true'
  
  use-project-prettier:
    description: 'Use project Prettier config if exists'
    required: false
    default: 'true'
  
  skip-playwright:
    description: 'Skip Playwright tests'
    required: false
    default: 'false'
  
  skip-eslint:
    description: 'Skip ESLint checks'
    required: false
    default: 'false'
  
  skip-prettier:
    description: 'Skip Prettier checks'
    required: false
    default: 'false'
  
  fail-on-error:
    description: 'Fail action on any error'
    required: false
    default: 'false'

outputs:
  test-results:
    description: 'Comprehensive JSON summary of GUI test results'
    value: ${{ steps.test-summary.outputs.results }}
  
  visual-artifacts-path:
    description: 'Path to generated visual artifacts and screenshots'
    value: ${{ steps.setup.outputs.artifacts-path }}
  
  dashboard-url:
    description: 'URL to the deployed visual testing dashboard'
    value: ${{ steps.deploy-report.outputs.page_url }}
  
  test-pass-rate:
    description: 'GUI test pass rate percentage'
    value: ${{ steps.test-summary.outputs.pass-rate }}
  
  total-gui-tests:
    description: 'Total number of GUI tests executed'
    value: ${{ steps.test-summary.outputs.total-tests }}
  
  code-quality-score:
    description: 'Overall code quality score based on linting results'
    value: ${{ steps.lint-summary.outputs.quality-score }}
  
  pr-test-results:
    description: 'JSON summary of PR branch GUI test results'
    value: ${{ steps.test-summary.outputs.pr-results }}
  
  main-test-results:
    description: 'JSON summary of main branch GUI test results'
    value: ${{ steps.test-summary.outputs.main-results }}
  
  visual-comparison:
    description: 'Visual comparison analysis between PR and main branch'
    value: ${{ steps.test-summary.outputs.visual-comparison }}
  
  gui-regression-detected:
    description: 'Whether GUI test regression was detected in PR'
    value: ${{ steps.test-summary.outputs.regression-detected }}
  
  review-checklist-status:
    description: 'Status of the code review checklist completion'
    value: ${{ steps.checklist.outputs.status }}
  
  artifacts-uploaded:
    description: 'List of uploaded artifacts (modular mode)'
    value: ${{ steps.artifacts-status.outputs.artifacts }}
  
  lint-results:
    description: 'Lint results summary JSON (modular mode)'
    value: ${{ steps.lint-summary.outputs.results }}

runs:
  using: 'composite'
  steps:
    - name: Setup GUI testing environment
      id: setup
      shell: bash
      run: |
        echo "ğŸš€ Setting up GUI-Based Testing Code Review environment..."
        mkdir -p ${{ inputs.custom-artifacts-path }}
        echo "artifacts-path=${{ inputs.custom-artifacts-path }}" >> $GITHUB_OUTPUT
        echo "âœ… Environment ready for visual GUI test analysis"
    
    - name: Setup Node.js for GUI test tools
      uses: actions/setup-node@v4
      with:
        node-version: ${{ inputs.node-version }}
    
    - name: Install GUI testing dependencies
      shell: bash
      run: |
        echo "ğŸ“¦ Installing dependencies for GUI test analysis..."
        if [ -f "package.json" ]; then
          npm install
          if [ -n "${{ inputs.dependencies }}" ]; then
            echo "ğŸ“¦ Installing additional dependencies: ${{ inputs.dependencies }}"
            npm install ${{ inputs.dependencies }}
          fi
          echo "âœ… Dependencies installed successfully"
        else
          echo "âš ï¸ No package.json found, installing minimal dependencies..."
          npm init -y
          npm install @playwright/test@^1.52.0 @mermaid-js/mermaid-cli@10.6.1 @octokit/core@^5.0.0 marked@15.0.12 prettier@^3.3.2 eslint@^8.0.0
          
          if [ -n "${{ inputs.dependencies }}" ]; then
            echo "ğŸ“¦ Installing additional dependencies: ${{ inputs.dependencies }}"
            npm install ${{ inputs.dependencies }}
          fi
          echo "âœ… Dependencies installed"
        fi
    
    # Download existing artifacts if in dashboard-only mode
    - name: Download existing artifacts
      if: inputs.mode == 'dashboard-only' || inputs.mode == 'comment-only'
      shell: bash
      run: |
        echo "ğŸ“¥ Checking for existing artifacts..."
        
        # Create artifacts directory
        mkdir -p ${{ inputs.custom-artifacts-path }}
        
        # Function to copy playwright artifact if it exists
        if [ -n "${{ inputs.playwright-artifact }}" ]; then
          echo "ğŸ“¥ Using Playwright artifact: ${{ inputs.playwright-artifact }}"
          if [ -d "${{ inputs.playwright-artifact }}" ]; then
            cp -r ${{ inputs.playwright-artifact }}/* ${{ inputs.custom-artifacts-path }}/
            echo "âœ… Copied Playwright artifacts"
          else
            echo "âš ï¸ Playwright artifact directory not found: ${{ inputs.playwright-artifact }}"
          fi
        fi
        
        # Function to copy ESLint artifact if it exists
        if [ -n "${{ inputs.eslint-artifact }}" ]; then
          echo "ğŸ“¥ Using ESLint artifact: ${{ inputs.eslint-artifact }}"
          if [ -d "${{ inputs.eslint-artifact }}" ]; then
            cp -r ${{ inputs.eslint-artifact }}/* ${{ inputs.custom-artifacts-path }}/
            echo "âœ… Copied ESLint artifacts"
          else
            echo "âš ï¸ ESLint artifact directory not found: ${{ inputs.eslint-artifact }}"
          fi
        fi
        
        # Function to copy Prettier artifact if it exists
        if [ -n "${{ inputs.prettier-artifact }}" ]; then
          echo "ğŸ“¥ Using Prettier artifact: ${{ inputs.prettier-artifact }}"
          if [ -d "${{ inputs.prettier-artifact }}" ]; then
            cp -r ${{ inputs.prettier-artifact }}/* ${{ inputs.custom-artifacts-path }}/
            echo "âœ… Copied Prettier artifacts"
          else
            echo "âš ï¸ Prettier artifact directory not found: ${{ inputs.prettier-artifact }}"
          fi
        fi
    
    - name: Install Playwright browsers for GUI testing
      if: inputs.mode == 'full' || inputs.mode == 'test-only'
      shell: bash
      run: |
        echo "ğŸ­ Installing Playwright browsers for GUI testing..."
        npx playwright install --with-deps
        echo "âœ… Playwright browsers ready for GUI test execution"
    
    - name: Setup reviewdog for code quality feedback
      if: (inputs.mode == 'full' || inputs.mode == 'lint-only') && github.event_name == 'pull_request'
      uses: reviewdog/action-setup@v1
      with:
        reviewdog_version: latest
    
    - name: Run code quality analysis with visual feedback
      id: lint
      if: inputs.mode == 'full' || inputs.mode == 'lint-only'
      shell: bash
      env:
        GITHUB_TOKEN: ${{ inputs.github-token }}
        GITHUB_EVENT_NAME: ${{ github.event_name }}
        REVIEWDOG_GITHUB_API_TOKEN: ${{ inputs.github-token }}
        REVIEWDOG_REPORTER: ${{ inputs.reviewdog-reporter }}
        USE_PROJECT_ESLINT: ${{ inputs.use-project-eslint }}
        USE_PROJECT_PRETTIER: ${{ inputs.use-project-prettier }}
        SKIP_ESLINT: ${{ inputs.skip-eslint }}
        SKIP_PRETTIER: ${{ inputs.skip-prettier }}
      run: |
        echo "ğŸ” Running code quality analysis for GUI tests..."
        if [ -f "${{ github.action_path }}/scripts/lint.js" ]; then
          node "${{ github.action_path }}/scripts/lint.js"
        elif [ -f "scripts/lint.js" ]; then
          node scripts/lint.js
        else
          echo "âš ï¸ Lint script not found, skipping code quality analysis"
        fi
        echo "âœ… Code quality analysis completed"
      continue-on-error: ${{ inputs.fail-on-error != 'true' }}
    
    - name: Execute GUI tests on PR branch
      id: gui-tests-pr
      if: inputs.mode == 'full' || inputs.mode == 'test-only'
      shell: bash
      env:
        PLAYWRIGHT_CONFIG: ${{ inputs.playwright-config }}
        TEST_FILES: ${{ inputs.test-files }}
        SKIP_PLAYWRIGHT: ${{ inputs.skip-playwright }}
      run: |
        if [ "$SKIP_PLAYWRIGHT" == "true" ]; then
          echo "â© Skipping Playwright tests as requested"
          exit 0
        fi
        
        echo "ğŸ§ª Executing GUI tests on PR branch..."
        if [ -f "${{ github.action_path }}/scripts/playwright-test.js" ]; then
          node "${{ github.action_path }}/scripts/playwright-test.js"
        elif [ -f "scripts/playwright-test.js" ]; then
          node scripts/playwright-test.js
        else
          echo "ğŸ­ Running Playwright tests directly..."
          npx playwright test ${{ inputs.test-files }}
        fi
        echo "âœ… PR branch GUI tests completed"
      continue-on-error: ${{ inputs.fail-on-error != 'true' }}
    
    - name: Archive PR GUI test results and screenshots
      if: inputs.mode == 'full' || inputs.mode == 'test-only'
      shell: bash
      run: |
        echo "ğŸ“ Archiving PR branch GUI test artifacts..."
        mkdir -p ${{ inputs.custom-artifacts-path }}
        if [[ -f playwright-metrics.json ]]; then
          cp playwright-metrics.json ${{ inputs.custom-artifacts-path }}/playwright-summary.json
          cp playwright-metrics.json ${{ inputs.custom-artifacts-path }}/playwright-summary-pr.json
          echo "âœ… PR test summary archived"
        else
          echo "{}" > ${{ inputs.custom-artifacts-path }}/playwright-summary-pr.json
          echo "âš ï¸ No PR summary found, created empty file"
        fi
        
        # Archive HTML report with screenshots
        mkdir -p ${{ inputs.custom-artifacts-path }}/pr-report
        if [ -d "playwright-report" ] && [ "$(ls -A playwright-report)" ]; then
          cp -r playwright-report/* ${{ inputs.custom-artifacts-path }}/pr-report/ || echo "No PR report contents to move"
          echo "âœ… PR GUI test report with screenshots archived"
        else
          echo "âš ï¸ No PR report found to archive"
        fi
      continue-on-error: true
    
    - name: Execute GUI tests on main branch for visual comparison
      id: gui-tests-main
      if: inputs.enable-visual-comparison == 'true' && github.event_name == 'pull_request' && (inputs.mode == 'full' || inputs.mode == 'test-only')
      shell: bash
      env:
        MAIN_BRANCH: ${{ inputs.main-branch }}
        KEY_TEST_FILE: ${{ inputs.key-test-file }}
        SKIP_PLAYWRIGHT: ${{ inputs.skip-playwright }}
      run: |
        if [ "$SKIP_PLAYWRIGHT" == "true" ]; then
          echo "â© Skipping main branch tests as requested"
          exit 0
        fi
        
        echo "ğŸ”„ Checking out main branch GUI tests for comparison..."
        
        # Checkout main branch test files for comparison
        git checkout origin/$MAIN_BRANCH -- tests/ playwright.config.js || true
        
        # Verify successful checkout
        if [ ! -f "$KEY_TEST_FILE" ]; then
          echo "âš ï¸ Key test file '$KEY_TEST_FILE' not found. Skipping main branch comparison."
          mkdir -p playwright-report
          echo "{}" > ${{ inputs.custom-artifacts-path }}/playwright-summary-main.json
        else
          echo "âœ… Main branch test files checked out successfully"
          echo "ğŸ§ª Running GUI tests on main branch version..."
          
          if [ -f "${{ github.action_path }}/scripts/playwright-test.js" ]; then
            node "${{ github.action_path }}/scripts/playwright-test.js"
          elif [ -f "scripts/playwright-test.js" ]; then
            node scripts/playwright-test.js
          else
            npx playwright test ${{ inputs.test-files }}
          fi
          echo "âœ… Main branch GUI tests completed"
        fi
      continue-on-error: true
    
    - name: Archive main branch GUI test results for comparison
      if: inputs.enable-visual-comparison == 'true' && github.event_name == 'pull_request' && (inputs.mode == 'full' || inputs.mode == 'test-only')
      shell: bash
      run: |
        echo "ğŸ“ Archiving main branch GUI test artifacts..."
        if [[ -f playwright-metrics.json ]]; then
          cp playwright-metrics.json ${{ inputs.custom-artifacts-path }}/playwright-summary-main.json
          echo "âœ… Main branch test summary archived"
        else
          echo "{}" > ${{ inputs.custom-artifacts-path }}/playwright-summary-main.json
          echo "âš ï¸ No main branch summary found, created empty file"
        fi
        
        # Archive main branch HTML report
        mkdir -p ${{ inputs.custom-artifacts-path }}/main-report
        if [ -d "playwright-report" ] && [ "$(ls -A playwright-report)" ]; then
          cp -r playwright-report/* ${{ inputs.custom-artifacts-path }}/main-report/ || echo "No main branch report contents to move"
          echo "âœ… Main branch GUI test report archived"
        else
          echo "âš ï¸ No main branch report found to archive"
        fi
      continue-on-error: true
    
    - name: Restore PR test files
      if: inputs.enable-visual-comparison == 'true' && github.event_name == 'pull_request' && (inputs.mode == 'full' || inputs.mode == 'test-only')
      shell: bash
      run: |
        echo "ğŸ”„ Restoring PR branch test files..."
        git checkout HEAD -- tests/ playwright.config.js
        echo "âœ… PR test files restored"
      continue-on-error: true
    
    - name: Generate visual flow chart of GUI tests
      id: flowchart
      if: inputs.mode == 'full' || inputs.mode == 'dashboard-only'
      shell: bash
      run: |
        echo "ğŸ“Š Generating visual flowchart of GUI test execution..."
        if [ -f "${{ github.action_path }}/scripts/generate-flowchart.js" ]; then
          node "${{ github.action_path }}/scripts/generate-flowchart.js"
        elif [ -f "scripts/generate-flowchart.js" ]; then
          node scripts/generate-flowchart.js
        else
          echo "âš ï¸ Flowchart script not found, skipping visualization"
        fi
        echo "âœ… Visual flowchart generation completed"
      continue-on-error: true
    
    - name: Build review checklist for code reviewers
      id: checklist
      if: inputs.mode == 'full' || inputs.mode == 'dashboard-only'
      shell: bash
      run: |
        echo "ğŸ“‹ Building comprehensive review checklist..."
        if [ -f "${{ github.action_path }}/scripts/checklist.js" ]; then
          node "${{ github.action_path }}/scripts/checklist.js"
        elif [ -f "scripts/checklist.js" ]; then
          node scripts/checklist.js
        else
          echo "âš ï¸ Checklist script not found, skipping checklist generation"
        fi
        
        # Set checklist status output
        if [ -f "${{ inputs.custom-artifacts-path }}/checklist.json" ]; then
          echo "status=completed" >> $GITHUB_OUTPUT
        else
          echo "status=incomplete" >> $GITHUB_OUTPUT
        fi
        echo "âœ… Review checklist generation completed"
      continue-on-error: true
    
    - name: Build visual dashboard for code reviewers
      id: dashboard
      if: inputs.mode == 'full' || inputs.mode == 'dashboard-only'
      shell: bash
      env:
        ARTIFACTS_DIR: ${{ inputs.custom-artifacts-path }}
      run: |
        echo "ğŸ¨ Building interactive visual dashboard..."
        if [ -f "${{ github.action_path }}/scripts/generate-webpage.js" ]; then
          node "${{ github.action_path }}/scripts/generate-webpage.js"
        elif [ -f "scripts/generate-webpage.js" ]; then
          node scripts/generate-webpage.js
        else
          echo "âš ï¸ Dashboard script not found, skipping dashboard generation"
        fi
        echo "âœ… Visual dashboard generation completed"
      continue-on-error: true
    
    - name: Create comprehensive test summary with visual comparison
      id: test-summary
      if: inputs.mode == 'full' || inputs.mode == 'dashboard-only'
      shell: bash
      run: |
        echo "ğŸ“Š Creating comprehensive test summary with visual analysis..."
        
        # Initialize variables for analysis
        PR_TOTAL=0; PR_PASSED=0; PR_FAILED=0; PR_SKIPPED=0; PR_RATE=0
        MAIN_TOTAL=0; MAIN_PASSED=0; MAIN_FAILED=0; MAIN_SKIPPED=0; MAIN_RATE=0
        REGRESSION="false"
        
        # Analyze PR branch results
        if [ -f "${{ inputs.custom-artifacts-path }}/playwright-summary-pr.json" ]; then
          if command -v jq &> /dev/null; then
            PR_TOTAL=$(jq -r '.total // 0' ${{ inputs.custom-artifacts-path }}/playwright-summary-pr.json)
            PR_PASSED=$(jq -r '.passed // 0' ${{ inputs.custom-artifacts-path }}/playwright-summary-pr.json)
            PR_FAILED=$(jq -r '.failed // 0' ${{ inputs.custom-artifacts-path }}/playwright-summary-pr.json)
            PR_SKIPPED=$(jq -r '.skipped // 0' ${{ inputs.custom-artifacts-path }}/playwright-summary-pr.json)
            PR_RATE=$(jq -r '.pass_rate // 0' ${{ inputs.custom-artifacts-path }}/playwright-summary-pr.json)
            
            echo "PR GUI Tests: $PR_TOTAL total, $PR_PASSED passed, $PR_FAILED failed ($PR_RATE% pass rate)"
          fi
        fi
        
        # Analyze main branch results for comparison
        if [ -f "${{ inputs.custom-artifacts-path }}/playwright-summary-main.json" ] && [ "${{ inputs.enable-visual-comparison }}" = "true" ]; then
          if command -v jq &> /dev/null; then
            MAIN_TOTAL=$(jq -r '.total // 0' ${{ inputs.custom-artifacts-path }}/playwright-summary-main.json)
            MAIN_PASSED=$(jq -r '.passed // 0' ${{ inputs.custom-artifacts-path }}/playwright-summary-main.json)
            MAIN_FAILED=$(jq -r '.failed // 0' ${{ inputs.custom-artifacts-path }}/playwright-summary-main.json)
            MAIN_SKIPPED=$(jq -r '.skipped // 0' ${{ inputs.custom-artifacts-path }}/playwright-summary-main.json)
            MAIN_RATE=$(jq -r '.pass_rate // 0' ${{ inputs.custom-artifacts-path }}/playwright-summary-main.json)
            
            echo "Main GUI Tests: $MAIN_TOTAL total, $MAIN_PASSED passed, $MAIN_FAILED failed ($MAIN_RATE% pass rate)"
            
            # Detect visual regression
            if [ "$PR_FAILED" -gt "$MAIN_FAILED" ] || [ $(echo "$PR_RATE < $MAIN_RATE" | bc -l 2>/dev/null || echo "0") -eq 1 ]; then
              REGRESSION="true"
              echo "âš ï¸ GUI test regression detected!"
            fi
          fi
        fi
        
        # Create visual comparison summary
        VISUAL_COMPARISON="{\"pr\":{\"total\":$PR_TOTAL,\"passed\":$PR_PASSED,\"failed\":$PR_FAILED,\"skipped\":$PR_SKIPPED,\"pass_rate\":$PR_RATE},\"main\":{\"total\":$MAIN_TOTAL,\"passed\":$MAIN_PASSED,\"failed\":$MAIN_FAILED,\"skipped\":$MAIN_SKIPPED,\"pass_rate\":$MAIN_RATE},\"regression_detected\":$REGRESSION}"
        
        # Set comprehensive outputs
        echo "results={\"total\":$PR_TOTAL,\"passed\":$PR_PASSED,\"failed\":$PR_FAILED,\"pass_rate\":$PR_RATE}" >> $GITHUB_OUTPUT
        echo "pr-results={\"total\":$PR_TOTAL,\"passed\":$PR_PASSED,\"failed\":$PR_FAILED,\"skipped\":$PR_SKIPPED,\"pass_rate\":$PR_RATE}" >> $GITHUB_OUTPUT
        echo "main-results={\"total\":$MAIN_TOTAL,\"passed\":$MAIN_PASSED,\"failed\":$MAIN_FAILED,\"skipped\":$MAIN_SKIPPED,\"pass_rate\":$MAIN_RATE}" >> $GITHUB_OUTPUT
        echo "visual-comparison=$VISUAL_COMPARISON" >> $GITHUB_OUTPUT
        echo "regression-detected=$REGRESSION" >> $GITHUB_OUTPUT
        echo "total-tests=$PR_TOTAL" >> $GITHUB_OUTPUT
        echo "pass-rate=$PR_RATE" >> $GITHUB_OUTPUT
        
        echo "âœ… Comprehensive visual test analysis completed"
      continue-on-error: true
    
    - name: Extract code quality metrics
      id: lint-summary
      if: inputs.mode == 'full' || inputs.mode == 'dashboard-only'
      shell: bash
      run: |
        echo "ğŸ“ˆ Extracting code quality metrics..."
        QUALITY_SCORE=100
        
        if [ -f "${{ inputs.custom-artifacts-path }}/lint-summary.json" ]; then
          if command -v jq &> /dev/null; then
            ESLINT_ERRORS=$(jq -r '.eslint.errors // 0' ${{ inputs.custom-artifacts-path }}/lint-summary.json)
            PRETTIER_ISSUES=$(jq -r '.prettier.filesWithIssues // 0' ${{ inputs.custom-artifacts-path }}/lint-summary.json)
            
            # Calculate quality score (simple algorithm)
            QUALITY_SCORE=$((100 - ESLINT_ERRORS * 5 - PRETTIER_ISSUES * 2))
            if [ $QUALITY_SCORE -lt 0 ]; then QUALITY_SCORE=0; fi
            
            echo "Code Quality Score: $QUALITY_SCORE/100"
          fi
        fi
        
        echo "quality-score=$QUALITY_SCORE" >> $GITHUB_OUTPUT
        echo "results={\"quality_score\":$QUALITY_SCORE,\"eslint_errors\":$ESLINT_ERRORS,\"prettier_issues\":$PRETTIER_ISSUES}" >> $GITHUB_OUTPUT
        echo "âœ… Code quality metrics extracted"
      continue-on-error: true
    
    - name: Capture artifacts status
      id: artifacts-status
      shell: bash
      run: |
        echo "ğŸ“Š Listing generated artifacts..."
        ARTIFACTS_LIST=$(find ${{ inputs.custom-artifacts-path }} -type f -name "*.json" | sort | tr '\n' ',' | sed 's/,$//')
        echo "artifacts=$ARTIFACTS_LIST" >> $GITHUB_OUTPUT
        echo "âœ… Artifacts captured: $ARTIFACTS_LIST"
      continue-on-error: true
    
    - name: Upload visual artifacts for code reviewers
      uses: actions/upload-artifact@v4
      with:
        name: gui-test-visual-artifacts
        path: ${{ inputs.custom-artifacts-path }}/
        retention-days: ${{ inputs.artifacts-retention-days }}
      continue-on-error: true
    
    - name: Deploy visual dashboard to GitHub Pages
      id: deploy-report
      if: inputs.enable-github-pages == 'true' && (inputs.mode == 'full' || inputs.mode == 'dashboard-only')
      uses: actions/deploy-pages@v4
      with:
        artifact_name: gui-test-visual-artifacts
        path: ${{ inputs.custom-artifacts-path }}/web-report
      continue-on-error: true
    
    - name: Post comprehensive visual feedback to PR
      if: inputs.enable-pr-comments == 'true' && github.event_name == 'pull_request' && (inputs.mode == 'full' || inputs.mode == 'dashboard-only' || inputs.mode == 'comment-only')
      shell: bash
      env:
        GITHUB_TOKEN: ${{ inputs.github-token }}
        ARTIFACTS_DIR: ${{ inputs.custom-artifacts-path }}
        WEB_REPORT_URL: ${{ inputs.web-report-url || steps.deploy-report.outputs.page_url || format('https://{0}.github.io/{1}/', github.repository_owner, github.event.repository.name) }}
      run: |
        echo "ğŸ’¬ Posting comprehensive visual feedback to PR..."
        if [ -f "${{ github.action_path }}/scripts/summary-comment.js" ]; then
          node "${{ github.action_path }}/scripts/summary-comment.js"
        elif [ -f "scripts/summary-comment.js" ]; then
          node scripts/summary-comment.js
        else
          echo "âš ï¸ Summary comment script not found, skipping PR comment"
        fi
        echo "âœ… Visual feedback posted to PR"
      continue-on-error: true
    
    - name: Validate GUI test results
      if: inputs.fail-on-test-failure == 'true' && (inputs.mode == 'full' || inputs.mode == 'test-only')
      shell: bash
      run: |
        echo "ğŸ” Validating GUI test results..."
        if [ -f "${{ inputs.custom-artifacts-path }}/playwright-summary-pr.json" ]; then
          if command -v jq &> /dev/null; then
            FAILED=$(jq -r '.failed // 0' ${{ inputs.custom-artifacts-path }}/playwright-summary-pr.json)
            if [ "$FAILED" -gt 0 ]; then
              echo "âŒ GUI tests failed: $FAILED test failures detected"
              echo "ğŸ” Check the visual dashboard for detailed analysis"
              exit 1
            fi
          fi
        fi
        echo "âœ… All GUI tests passed successfully"
    
    - name: Final summary for code reviewers
      if: always()
      shell: bash
      run: |
        echo ""
        echo "ğŸ‰ GUI-Based Testing Code Review Analysis Complete!"
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
        echo "ğŸ“Š Visual artifacts and comprehensive feedback generated"
        echo "ğŸ¨ Interactive dashboard deployed for code reviewers"
        echo "ğŸ’¬ PR comments updated with visual context"
        echo "ğŸ“‹ Review checklist created for quality assurance"
        echo ""
        echo "ğŸ” Code reviewers can now:"
        echo "  â€¢ View visual test execution flow"
        echo "  â€¢ Compare PR vs main branch GUI test results"
        echo "  â€¢ Access comprehensive code quality feedback"
        echo "  â€¢ Review generated screenshots and artifacts"
        echo ""
        if [ -f "${{ inputs.custom-artifacts-path }}/test-summary.txt" ]; then
          echo "ğŸ“ˆ Summary:"
          cat ${{ inputs.custom-artifacts-path }}/test-summary.txt
        fi
        echo "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•"
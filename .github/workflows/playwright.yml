name: GUI Test Review

on:
  push:
    branches: [main]
  pull_request:
    branches: [main]

jobs:
  lint:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      pull-requests: write
    
    steps:
      - uses: actions/checkout@v4
        with: 
          fetch-depth: 0
      
      - uses: actions/setup-node@v4
        with: { node-version: 18 }
      
      - name: Install dependencies
        run: npm install
      
      # Run your scripts for reporting first
      - name: Run existing lint script for reporting
        run: |
          # Create a simplified lint script that only generates reports
          cat > scripts/lint-for-report.js << 'EOF'
          #!/usr/bin/env node
          const { execSync } = require('child_process');
          const fs = require('fs');

          function runPrettier() {
            console.log('\n▶ Prettier check');
            let filesToFormat = [];
            try {
              execSync('npx prettier --check "tests/**/*.{js,ts,tsx,json}"', { stdio: 'inherit' });
            } catch (error) {
              const output = error.stdout?.toString() || '';
              const matches = output.match(/[^\s]+\.(js|ts|tsx|json)/g);
              if (matches) {
                filesToFormat = matches.filter(file => file.startsWith('tests/'));
              }
            }
            
            return {
              filesWithIssues: filesToFormat.length,
              files: filesToFormat
            };
          }

          function runESLint() {
            console.log('\n▶ ESLint');
            let raw = '';
            try {
              raw = execSync('npx eslint tests --ext .js,.ts,.tsx -f json', { encoding: 'utf8' });
            } catch (e) {
              raw = e.stdout?.toString() || '';
            }
            
            const results = raw ? JSON.parse(raw) : [];
            let errors = 0, warnings = 0, fixErr = 0, fixWarn = 0, first = '', files = new Set();

            results.forEach(f => {
              if (f.messages.length) files.add(f.filePath);
              f.messages.forEach(m => {
                if (m.severity === 2) {
                  errors++;
                  if (m.fix) fixErr++;
                  if (!first) first = `${m.ruleId || 'unknown-rule'} in ${f.filePath}:${m.line}`;
                } else if (m.severity === 1) {
                  warnings++;
                  if (m.fix) fixWarn++;
                }
              });
            });

            return {
              files: files.size,
              errors,
              warnings,
              fixableErrors: fixErr,
              fixableWarnings: fixWarn,
              first
            };
          }

          const prettier = runPrettier();
          const eslint = runESLint();

          fs.mkdirSync('artifacts', { recursive: true });
          fs.writeFileSync('artifacts/lint-summary.json', JSON.stringify({ prettier, eslint }, null, 2));
          console.log('📝 artifacts/lint-summary.json written');
          EOF
          
          chmod +x scripts/lint-for-report.js
          node scripts/lint-for-report.js
        continue-on-error: true
      
      - name: Playwright tests
        run: node scripts/playwright-test.js
        continue-on-error: true
      
      - name: Generate flow-chart
        run: node scripts/generate-flowchart.js
        continue-on-error: true
      
      - name: Build checklist
        run: node scripts/checklist.js
        continue-on-error: true
      
      - name: Build static HTML report
        run: node scripts/generate-webpage.js
        continue-on-error: true
      
      - name: Create test-summary badge
        run: |
          if [ -f "artifacts/playwright-summary.json" ]; then
            jq -r '"Total: \(.total) | Passed: \(.passed) | Failed: \(.failed) | Skipped: \(.skipped)"' \
              artifacts/playwright-summary.json > artifacts/test-summary.txt
          else
            echo "No playwright-summary.json file found. Creating empty summary."
            echo "Total: 0 | Passed: 0 | Failed: 0 | Skipped: 0" > artifacts/test-summary.txt
          fi
        continue-on-error: true
      
      # Let's use GitHub native annotations instead of reviewdog
      - name: Prettier Check and Annotations
        if: github.event_name == 'pull_request'
        run: |
          # Create dirs for temp files
          mkdir -p temp_files
          
          # Find files with Prettier issues
          PRETTIER_OUTPUT=$(npx prettier --check "tests/**/*.{js,ts,tsx,json}" 2>&1 || true)
          echo "$PRETTIER_OUTPUT"
          
          # Extract the list of files with issues
          FILES=$(echo "$PRETTIER_OUTPUT" | grep -oE "[^ ]+\.(js|ts|tsx|json)" | sort -u || echo "")
          
          if [ -n "$FILES" ]; then
            echo "Files with Prettier issues:"
            echo "$FILES"
            
            # Count total issues
            TOTAL_FILES=$(echo "$FILES" | wc -l)
            
            # For the first file, create fix suggestions
            FIRST_FILE=$(echo "$FILES" | head -n 1)
            if [ -n "$FIRST_FILE" ]; then
              echo "Analyzing formatting issues in: $FIRST_FILE"
              
              # Create formatted version
              FORMATTED=$(npx prettier "$FIRST_FILE")
              ORIGINAL=$(cat "$FIRST_FILE")
              
              # Generate limited number of annotations (max 10)
              ISSUES=0
              LINE=1
              ORIGINAL_LINES=()
              FORMATTED_LINES=()
              
              # Read original file line by line
              while IFS= read -r original_line; do
                ORIGINAL_LINES+=("$original_line")
              done <<< "$ORIGINAL"
              
              # Read formatted file line by line
              while IFS= read -r formatted_line; do
                FORMATTED_LINES+=("$formatted_line")
              done <<< "$FORMATTED"
              
              # Compare line by line and create annotations
              for i in "${!ORIGINAL_LINES[@]}"; do
                if [ "$ISSUES" -ge 8 ]; then
                  break  # Limit to 8 issues
                fi
                
                if [ "${ORIGINAL_LINES[$i]}" != "${FORMATTED_LINES[$i]}" ]; then
                  LINE_NUM=$((i+1))
                  
                  # Create GitHub annotation
                  echo "::warning file=$FIRST_FILE,line=$LINE_NUM::Formatting issue - should be: ${FORMATTED_LINES[$i]}"
                  
                  ISSUES=$((ISSUES+1))
                fi
              done
              
              # Add a summary comment
              if [ "$TOTAL_FILES" -gt 1 ]; then
                REMAINING=$((TOTAL_FILES - 1))
                
                # Create a GitHub step summary
                echo "## Prettier Issues" >> $GITHUB_STEP_SUMMARY
                echo "Found **$TOTAL_FILES files** with Prettier formatting issues." >> $GITHUB_STEP_SUMMARY
                echo "Showing a few suggestions for **$FIRST_FILE** only due to GitHub annotation limits." >> $GITHUB_STEP_SUMMARY
                echo "**$REMAINING more files** need formatting:" >> $GITHUB_STEP_SUMMARY
                echo '```' >> $GITHUB_STEP_SUMMARY
                echo "$FILES" | tail -n $REMAINING >> $GITHUB_STEP_SUMMARY
                echo '```' >> $GITHUB_STEP_SUMMARY
                echo "Run \`npx prettier --write 'tests/**/*.{js,ts,tsx,json}'\` locally to fix all issues." >> $GITHUB_STEP_SUMMARY
              fi
            fi
          else
            echo "No Prettier issues found."
          fi
        continue-on-error: true
      
      # Run ESLint with native GitHub annotations
      - name: ESLint Check with Annotations
        if: github.event_name == 'pull_request'
        run: |
          # Run ESLint with GitHub reporter
          npx eslint tests --ext .js,.ts,.tsx --format=github || true
          
          # Also run ESLint with JSON format to get total counts
          ESLINT_OUTPUT=$(npx eslint tests --ext .js,.ts,.tsx -f json || true)
          
          # Save for debugging
          mkdir -p temp_files
          echo "$ESLINT_OUTPUT" > temp_files/eslint-output.json
          
          # Count issues if we have valid JSON
          if echo "$ESLINT_OUTPUT" | jq empty 2>/dev/null; then
            TOTAL_ISSUES=$(echo "$ESLINT_OUTPUT" | jq '[.[].messages | length] | add')
            ERROR_COUNT=$(echo "$ESLINT_OUTPUT" | jq '[.[].messages | .[] | select(.severity == 2)] | length')
            WARNING_COUNT=$(echo "$ESLINT_OUTPUT" | jq '[.[].messages | .[] | select(.severity == 1)] | length')
            
            # GitHub step summary for ESLint
            echo "## ESLint Issues" >> $GITHUB_STEP_SUMMARY
            echo "Found **$TOTAL_ISSUES total issues** with ESLint." >> $GITHUB_STEP_SUMMARY
            echo "- **$ERROR_COUNT errors**" >> $GITHUB_STEP_SUMMARY
            echo "- **$WARNING_COUNT warnings**" >> $GITHUB_STEP_SUMMARY
            echo "GitHub shows only a limited number of annotations due to platform limits." >> $GITHUB_STEP_SUMMARY
            echo "Run \`npx eslint --fix tests\` locally to fix all auto-fixable issues." >> $GITHUB_STEP_SUMMARY
          fi
        continue-on-error: true
      
      - uses: actions/upload-artifact@v4
        with:
          name: gui-artifacts
          path: artifacts/*
        continue-on-error: true
  
  deploy-report:
    needs: lint
    runs-on: ubuntu-latest
    permissions:
      pages: write
      id-token: write
    environment:
      name: github-pages
      url: ${{ steps.deploy.outputs.page_url }}
    
    steps:
      - uses: actions/download-artifact@v4
        with: { name: gui-artifacts, path: gui-artifacts }
        continue-on-error: true
      
      - uses: actions/upload-pages-artifact@v3
        with: { path: gui-artifacts/web-report }
        continue-on-error: true
      
      - id: deploy
        uses: actions/deploy-pages@v4
        continue-on-error: true
  
  comment_link:
    if: github.event_name == 'pull_request'
    needs: deploy-report
    runs-on: ubuntu-latest
    permissions:
      pull-requests: write
    
    steps:
      - uses: actions/checkout@v4
        continue-on-error: true
      
      - uses: actions/setup-node@v4
        with: { node-version: 18 }
        continue-on-error: true
      
      - name: Install JS dependencies (Octokit, marked, etc.)
        run: npm install
        continue-on-error: true
      
      - uses: actions/download-artifact@v4
        with:
          name: gui-artifacts
          path: gui-artifacts
        continue-on-error: true
      
      - name: Post / update GUI-test summary comment
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          ARTIFACTS_DIR: gui-artifacts
          WEB_REPORT_URL: https://digitalproductinnovationanddevelopment.github.io/Code-Reviews-of-GUI-Tests/index.html
        run: node scripts/summary-comment.js
        continue-on-error: true